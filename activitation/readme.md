激活函数要求:单侧抑制,宽阔的兴奋域,稀疏激活

1.稀疏性在70%-85%之间比较好.过分的稀疏会减少模型的有效容量,导致模型无法学到有效特征.

relu好处:更快的收敛,缩小了是否进行预训练的代沟.


参考:http://www.cnblogs.com/neopenx/p/4453161.html
